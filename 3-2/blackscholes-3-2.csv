Learning Rate, Epochs, Sampling Rate, test data fraction, max layers, max neurons, Error
0.1,1,0.1,0.1,3,2,0.996204160
0.1,2,0.1,0.1,3,2,0.981316530
0.1,3,0.1,0.1,3,2,0.910746680
0.1,4,0.1,0.1,3,2,0.709986820
0.1,5,0.1,0.1,3,2,0.633237090
0.1,6,0.1,0.1,3,2,0.634363820
0.1,7,0.1,0.1,3,2,0.647995410
0.1,8,0.1,0.1,3,2,0.628030000
0.1,9,0.1,0.1,3,2,0.629124610
0.1,10,0.1,0.1,3,2,0.631814680
0.1,11,0.1,0.1,3,2,0.639567200
0.1,12,0.1,0.1,3,2,0.649194940
0.1,13,0.1,0.1,3,2,0.633049530
0.1,14,0.1,0.1,3,2,0.637002800
0.1,15,0.1,0.1,3,2,0.640402790
0.1,16,0.1,0.1,3,2,0.634890490
0.1,17,0.1,0.1,3,2,0.631387660
0.1,18,0.1,0.1,3,2,0.629222020
0.1,19,0.1,0.1,3,2,0.634948530
0.1,20,0.1,0.1,3,2,0.637472930
0.1,21,0.1,0.1,3,2,0.635399930
0.1,22,0.1,0.1,3,2,0.641033330
0.1,23,0.1,0.1,3,2,0.637263260
0.1,24,0.1,0.1,3,2,0.638610210
0.1,25,0.1,0.1,3,2,0.636562680
0.1,26,0.1,0.1,3,2,0.639851990
0.1,27,0.1,0.1,3,2,0.632244350
0.1,28,0.1,0.1,3,2,0.642413680
0.1,29,0.1,0.1,3,2,0.639143650
0.1,30,0.1,0.1,3,2,0.634984150
0.1,31,0.1,0.1,3,2,0.638553050
0.1,32,0.1,0.1,3,2,0.611198470
0.1,33,0.1,0.1,3,2,0.636181140
0.1,34,0.1,0.1,3,2,0.627219670
0.1,35,0.1,0.1,3,2,0.634166770
0.1,36,0.1,0.1,3,2,0.637033530
0.1,37,0.1,0.1,3,2,0.622594070
0.1,38,0.1,0.1,3,2,0.617509710
0.1,39,0.1,0.1,3,2,0.634351900
0.1,40,0.1,0.1,3,2,0.635919040
0.1,41,0.1,0.1,3,2,0.622449830
0.1,42,0.1,0.1,3,2,0.632971350
0.1,43,0.1,0.1,3,2,0.632806440
0.1,44,0.1,0.1,3,2,0.621691330
0.1,45,0.1,0.1,3,2,0.625235840
0.1,46,0.1,0.1,3,2,0.635234400
0.1,47,0.1,0.1,3,2,0.633390270
0.1,48,0.1,0.1,3,2,0.629374180
0.1,49,0.1,0.1,3,2,0.618656760
0.1,50,0.1,0.1,3,2,0.622617180
0.1,51,0.1,0.1,3,2,0.626188060
0.1,52,0.1,0.1,3,2,0.618728640
0.1,53,0.1,0.1,3,2,0.620041820
0.1,54,0.1,0.1,3,2,0.618137710
0.1,55,0.1,0.1,3,2,0.618270010
0.1,56,0.1,0.1,3,2,0.620864890
0.1,57,0.1,0.1,3,2,0.627768560
0.1,58,0.1,0.1,3,2,0.620023130
0.1,59,0.1,0.1,3,2,0.618316240
0.1,60,0.1,0.1,3,2,0.614488690
0.1,61,0.1,0.1,3,2,0.623672840
0.1,62,0.1,0.1,3,2,0.620230830
0.1,63,0.1,0.1,3,2,0.618374080
0.1,64,0.1,0.1,3,2,0.626223120
0.1,65,0.1,0.1,3,2,0.624399230
0.1,66,0.1,0.1,3,2,0.618723020
0.1,67,0.1,0.1,3,2,0.612376040
0.1,68,0.1,0.1,3,2,0.617584060
0.1,69,0.1,0.1,3,2,0.619158050
0.1,70,0.1,0.1,3,2,0.617903930
0.1,71,0.1,0.1,3,2,0.622248970
0.1,72,0.1,0.1,3,2,0.615238320
0.1,73,0.1,0.1,3,2,0.614064150
0.1,74,0.1,0.1,3,2,0.622954040
0.1,75,0.1,0.1,3,2,0.615152900
0.1,76,0.1,0.1,3,2,0.609802680
0.1,77,0.1,0.1,3,2,0.624663930
0.1,78,0.1,0.1,3,2,0.619007510
0.1,79,0.1,0.1,3,2,0.617600230
0.1,80,0.1,0.1,3,2,0.616740820
0.1,81,0.1,0.1,3,2,0.617398040
0.1,82,0.1,0.1,3,2,0.619660550
0.1,83,0.1,0.1,3,2,0.620678110
0.1,84,0.1,0.1,3,2,0.618713070
0.1,85,0.1,0.1,3,2,0.610975340
0.1,86,0.1,0.1,3,2,0.612487500
0.1,87,0.1,0.1,3,2,0.620492240
0.1,88,0.1,0.1,3,2,0.618573430
0.1,89,0.1,0.1,3,2,0.617050350
0.1,90,0.1,0.1,3,2,0.615515400
0.1,91,0.1,0.1,3,2,0.617036810
0.1,92,0.1,0.1,3,2,0.603248770
0.1,93,0.1,0.1,3,2,0.607503760
0.1,94,0.1,0.1,3,2,0.615389670
0.1,95,0.1,0.1,3,2,0.618735670
0.1,96,0.1,0.1,3,2,0.613698770
0.1,97,0.1,0.1,3,2,0.607887220
0.1,98,0.1,0.1,3,2,0.606810410
0.1,99,0.1,0.1,3,2,0.611147250
0.1,100,0.1,0.1,3,2,0.615860980
