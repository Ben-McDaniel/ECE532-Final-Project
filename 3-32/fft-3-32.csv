Learning Rate, Epochs, Sampling Rate, test data fraction, max layers, max neurons, Error
0.1,1,0.1,0.1,3,32,0.956878800
0.1,2,0.1,0.1,3,32,0.931607730
0.1,3,0.1,0.1,3,32,0.944678460
0.1,4,0.1,0.1,3,32,0.920065100
0.1,5,0.1,0.1,3,32,0.917691520
0.1,6,0.1,0.1,3,32,0.932849990
0.1,7,0.1,0.1,3,32,0.912939950
0.1,8,0.1,0.1,3,32,0.928073220
0.1,9,0.1,0.1,3,32,0.912739070
0.1,10,0.1,0.1,3,32,0.913035730
0.1,11,0.1,0.1,3,32,0.893287920
0.1,12,0.1,0.1,3,32,0.890756710
0.1,13,0.1,0.1,3,32,0.920155610
0.1,14,0.1,0.1,3,32,0.878850600
0.1,15,0.1,0.1,3,32,0.768095440
0.1,16,0.1,0.1,3,32,0.762575260
0.1,17,0.1,0.1,3,32,0.793958460
0.1,18,0.1,0.1,3,32,0.772533430
0.1,19,0.1,0.1,3,32,0.818399690
0.1,20,0.1,0.1,3,32,0.786834780
0.1,21,0.1,0.1,3,32,0.833003260
0.1,22,0.1,0.1,3,32,0.785488440
0.1,23,0.1,0.1,3,32,0.819214450
0.1,24,0.1,0.1,3,32,0.802586170
0.1,25,0.1,0.1,3,32,0.690554420
0.1,26,0.1,0.1,3,32,0.810054620
0.1,27,0.1,0.1,3,32,0.932697050
0.1,28,0.1,0.1,3,32,0.897481460
0.1,29,0.1,0.1,3,32,0.966756940
0.1,30,0.1,0.1,3,32,0.821131890
0.1,31,0.1,0.1,3,32,0.852164830
0.1,32,0.1,0.1,3,32,0.692738830
0.1,33,0.1,0.1,3,32,0.971431330
0.1,34,0.1,0.1,3,32,0.829443320
0.1,35,0.1,0.1,3,32,0.690397030
0.1,36,0.1,0.1,3,32,0.719789970
0.1,37,0.1,0.1,3,32,0.857623800
0.1,38,0.1,0.1,3,32,0.583564890
0.1,39,0.1,0.1,3,32,0.618452720
0.1,40,0.1,0.1,3,32,0.655102460
0.1,41,0.1,0.1,3,32,0.773519180
0.1,42,0.1,0.1,3,32,0.846230110
0.1,43,0.1,0.1,3,32,0.708961050
0.1,44,0.1,0.1,3,32,0.696388540
0.1,45,0.1,0.1,3,32,0.740439410
0.1,46,0.1,0.1,3,32,0.491727680
0.1,47,0.1,0.1,3,32,0.553337090
0.1,48,0.1,0.1,3,32,0.511872380
0.1,49,0.1,0.1,3,32,0.582690000
0.1,50,0.1,0.1,3,32,0.377419120
0.1,51,0.1,0.1,3,32,0.473190550
0.1,52,0.1,0.1,3,32,0.500579290
0.1,53,0.1,0.1,3,32,0.523422930
0.1,54,0.1,0.1,3,32,0.487887120
0.1,55,0.1,0.1,3,32,0.254152200
0.1,56,0.1,0.1,3,32,0.280292680
0.1,57,0.1,0.1,3,32,0.314432900
0.1,58,0.1,0.1,3,32,0.341970390
0.1,59,0.1,0.1,3,32,0.358788370
0.1,60,0.1,0.1,3,32,0.456904970
0.1,61,0.1,0.1,3,32,0.329871740
0.1,62,0.1,0.1,3,32,0.239948160
0.1,63,0.1,0.1,3,32,0.269030100
0.1,64,0.1,0.1,3,32,0.334017840
0.1,65,0.1,0.1,3,32,0.369052610
0.1,66,0.1,0.1,3,32,0.351376010
0.1,67,0.1,0.1,3,32,0.338775130
0.1,68,0.1,0.1,3,32,0.360340070
0.1,69,0.1,0.1,3,32,0.379681360
0.1,70,0.1,0.1,3,32,0.386198220
0.1,71,0.1,0.1,3,32,0.296808910
0.1,72,0.1,0.1,3,32,0.277090440
0.1,73,0.1,0.1,3,32,0.272717420
0.1,74,0.1,0.1,3,32,0.214669470
0.1,75,0.1,0.1,3,32,0.352117700
0.1,76,0.1,0.1,3,32,0.097029930
0.1,77,0.1,0.1,3,32,0.266564480
0.1,78,0.1,0.1,3,32,0.189665100
0.1,79,0.1,0.1,3,32,0.255045630
0.1,80,0.1,0.1,3,32,0.200531780
0.1,81,0.1,0.1,3,32,0.308085190
0.1,82,0.1,0.1,3,32,0.291422140
0.1,83,0.1,0.1,3,32,0.213058570
0.1,84,0.1,0.1,3,32,0.278056480
0.1,85,0.1,0.1,3,32,0.281549450
0.1,86,0.1,0.1,3,32,0.203107260
0.1,87,0.1,0.1,3,32,0.227081910
0.1,88,0.1,0.1,3,32,0.252091000
0.1,89,0.1,0.1,3,32,0.158216600
0.1,90,0.1,0.1,3,32,0.227712200
0.1,91,0.1,0.1,3,32,0.115211090
0.1,92,0.1,0.1,3,32,0.186835480
0.1,93,0.1,0.1,3,32,0.174605300
0.1,94,0.1,0.1,3,32,0.205770050
0.1,95,0.1,0.1,3,32,0.261092900
0.1,96,0.1,0.1,3,32,0.231332460
0.1,97,0.1,0.1,3,32,0.163469340
0.1,98,0.1,0.1,3,32,0.141523650
0.1,99,0.1,0.1,3,32,0.175761570
0.1,100,0.1,0.1,3,32,0.171282840
