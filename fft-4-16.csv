Learning Rate, Epochs, Sampling Rate, test data fraction, max layers, max neurons, Error
0.1,1,0.1,0.1,4,16,0.962398200
0.1,2,0.1,0.1,4,16,0.927476450
0.1,3,0.1,0.1,4,16,0.929461370
0.1,4,0.1,0.1,4,16,0.928436370
0.1,5,0.1,0.1,4,16,0.917353610
0.1,6,0.1,0.1,4,16,0.929979290
0.1,7,0.1,0.1,4,16,0.920577560
0.1,8,0.1,0.1,4,16,0.924364200
0.1,9,0.1,0.1,4,16,0.911999170
0.1,10,0.1,0.1,4,16,0.910445910
0.1,11,0.1,0.1,4,16,0.899038430
0.1,12,0.1,0.1,4,16,0.836093370
0.1,13,0.1,0.1,4,16,0.868405050
0.1,14,0.1,0.1,4,16,0.858145290
0.1,15,0.1,0.1,4,16,0.728102330
0.1,16,0.1,0.1,4,16,0.802970180
0.1,17,0.1,0.1,4,16,0.889441760
0.1,18,0.1,0.1,4,16,0.815256000
0.1,19,0.1,0.1,4,16,0.831125740
0.1,20,0.1,0.1,4,16,0.837323750
0.1,21,0.1,0.1,4,16,0.774075370
0.1,22,0.1,0.1,4,16,0.809591400
0.1,23,0.1,0.1,4,16,0.765844730
0.1,24,0.1,0.1,4,16,0.887812530
0.1,25,0.1,0.1,4,16,0.704155710
0.1,26,0.1,0.1,4,16,0.799538790
0.1,27,0.1,0.1,4,16,0.666885680
0.1,28,0.1,0.1,4,16,0.678128310
0.1,29,0.1,0.1,4,16,0.701750270
0.1,30,0.1,0.1,4,16,0.696680640
0.1,31,0.1,0.1,4,16,0.629328270
0.1,32,0.1,0.1,4,16,0.725345520
0.1,33,0.1,0.1,4,16,0.681020800
0.1,34,0.1,0.1,4,16,0.533847610
0.1,35,0.1,0.1,4,16,0.656737480
0.1,36,0.1,0.1,4,16,0.723366490
0.1,37,0.1,0.1,4,16,0.623854570
0.1,38,0.1,0.1,4,16,0.481247010
0.1,39,0.1,0.1,4,16,0.628244720
0.1,40,0.1,0.1,4,16,0.579612820
0.1,41,0.1,0.1,4,16,0.633352020
0.1,42,0.1,0.1,4,16,0.481426880
0.1,43,0.1,0.1,4,16,0.567413680
0.1,44,0.1,0.1,4,16,0.456755200
0.1,45,0.1,0.1,4,16,0.499058130
0.1,46,0.1,0.1,4,16,0.427305140
0.1,47,0.1,0.1,4,16,0.543817880
0.1,48,0.1,0.1,4,16,0.520301990
0.1,49,0.1,0.1,4,16,0.502222310
0.1,50,0.1,0.1,4,16,0.478128590
0.1,51,0.1,0.1,4,16,0.622988360
0.1,52,0.1,0.1,4,16,0.555416880
0.1,53,0.1,0.1,4,16,0.508157580
0.1,54,0.1,0.1,4,16,0.340541410
0.1,55,0.1,0.1,4,16,0.484666730
0.1,56,0.1,0.1,4,16,0.292407520
0.1,57,0.1,0.1,4,16,0.367231820
0.1,58,0.1,0.1,4,16,0.527253880
0.1,59,0.1,0.1,4,16,0.441726440
0.1,60,0.1,0.1,4,16,0.366799030
0.1,61,0.1,0.1,4,16,0.374357100
0.1,62,0.1,0.1,4,16,0.236641560
0.1,63,0.1,0.1,4,16,0.326959280
0.1,64,0.1,0.1,4,16,0.354776800
0.1,65,0.1,0.1,4,16,0.343037550
0.1,66,0.1,0.1,4,16,0.359511210
0.1,67,0.1,0.1,4,16,0.329282830
0.1,68,0.1,0.1,4,16,0.328256500
0.1,69,0.1,0.1,4,16,0.281805960
0.1,70,0.1,0.1,4,16,0.270265060
0.1,71,0.1,0.1,4,16,0.260996400
0.1,72,0.1,0.1,4,16,0.197197390
0.1,73,0.1,0.1,4,16,0.320960580
0.1,74,0.1,0.1,4,16,0.259996010
0.1,75,0.1,0.1,4,16,0.292128390
0.1,76,0.1,0.1,4,16,0.234958620
0.1,77,0.1,0.1,4,16,0.300687130
0.1,78,0.1,0.1,4,16,0.158600140
0.1,79,0.1,0.1,4,16,0.180661490
0.1,80,0.1,0.1,4,16,0.317238280
0.1,81,0.1,0.1,4,16,0.284602420
0.1,82,0.1,0.1,4,16,0.286531230
0.1,83,0.1,0.1,4,16,0.250919390
0.1,84,0.1,0.1,4,16,0.183947400
0.1,85,0.1,0.1,4,16,0.248127080
0.1,86,0.1,0.1,4,16,0.214383720
0.1,87,0.1,0.1,4,16,0.199096800
0.1,88,0.1,0.1,4,16,0.255060010
0.1,89,0.1,0.1,4,16,0.280171550
0.1,90,0.1,0.1,4,16,0.248813040
0.1,91,0.1,0.1,4,16,0.182889130
0.1,92,0.1,0.1,4,16,0.212598640
0.1,93,0.1,0.1,4,16,0.216992420
0.1,94,0.1,0.1,4,16,0.076955050
0.1,95,0.1,0.1,4,16,0.214652300
0.1,96,0.1,0.1,4,16,0.210239150
0.1,97,0.1,0.1,4,16,0.184116070
0.1,98,0.1,0.1,4,16,0.223314400
0.1,99,0.1,0.1,4,16,0.226558300
0.1,100,0.1,0.1,4,16,0.178374120
