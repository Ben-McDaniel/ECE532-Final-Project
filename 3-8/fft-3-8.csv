Learning Rate, Epochs, Sampling Rate, test data fraction, max layers, max neurons, Error
0.1,1,0.1,0.1,3,8,0.950107530
0.1,2,0.1,0.1,3,8,0.927843920
0.1,3,0.1,0.1,3,8,0.929973760
0.1,4,0.1,0.1,3,8,0.919896560
0.1,5,0.1,0.1,3,8,0.922648430
0.1,6,0.1,0.1,3,8,0.923735290
0.1,7,0.1,0.1,3,8,0.924302570
0.1,8,0.1,0.1,3,8,0.911949290
0.1,9,0.1,0.1,3,8,0.907713280
0.1,10,0.1,0.1,3,8,0.906740680
0.1,11,0.1,0.1,3,8,0.897227800
0.1,12,0.1,0.1,3,8,0.893222650
0.1,13,0.1,0.1,3,8,0.885421900
0.1,14,0.1,0.1,3,8,0.860703310
0.1,15,0.1,0.1,3,8,0.878220500
0.1,16,0.1,0.1,3,8,0.846122200
0.1,17,0.1,0.1,3,8,0.759262080
0.1,18,0.1,0.1,3,8,0.844909860
0.1,19,0.1,0.1,3,8,0.782520680
0.1,20,0.1,0.1,3,8,0.822190610
0.1,21,0.1,0.1,3,8,0.812273560
0.1,22,0.1,0.1,3,8,0.835384660
0.1,23,0.1,0.1,3,8,0.771752740
0.1,24,0.1,0.1,3,8,0.871825620
0.1,25,0.1,0.1,3,8,0.896274420
0.1,26,0.1,0.1,3,8,0.815935450
0.1,27,0.1,0.1,3,8,0.812695730
0.1,28,0.1,0.1,3,8,0.832727140
0.1,29,0.1,0.1,3,8,0.845180500
0.1,30,0.1,0.1,3,8,0.757736380
0.1,31,0.1,0.1,3,8,0.899321720
0.1,32,0.1,0.1,3,8,0.937262970
0.1,33,0.1,0.1,3,8,0.756808550
0.1,34,0.1,0.1,3,8,0.737225350
0.1,35,0.1,0.1,3,8,0.728383260
0.1,36,0.1,0.1,3,8,0.681423900
0.1,37,0.1,0.1,3,8,0.766265100
0.1,38,0.1,0.1,3,8,0.726441170
0.1,39,0.1,0.1,3,8,0.771918740
0.1,40,0.1,0.1,3,8,0.871169500
0.1,41,0.1,0.1,3,8,0.764703940
0.1,42,0.1,0.1,3,8,0.665950920
0.1,43,0.1,0.1,3,8,0.674524070
0.1,44,0.1,0.1,3,8,0.735365960
0.1,45,0.1,0.1,3,8,0.539276580
0.1,46,0.1,0.1,3,8,0.634364160
0.1,47,0.1,0.1,3,8,0.712524010
0.1,48,0.1,0.1,3,8,0.530209620
0.1,49,0.1,0.1,3,8,0.533010570
0.1,50,0.1,0.1,3,8,0.677014830
0.1,51,0.1,0.1,3,8,0.507695420
0.1,52,0.1,0.1,3,8,0.561659760
0.1,53,0.1,0.1,3,8,0.559962010
0.1,54,0.1,0.1,3,8,0.431310150
0.1,55,0.1,0.1,3,8,0.553705740
0.1,56,0.1,0.1,3,8,0.553888960
0.1,57,0.1,0.1,3,8,0.386312060
0.1,58,0.1,0.1,3,8,0.477732990
0.1,59,0.1,0.1,3,8,0.403334920
0.1,60,0.1,0.1,3,8,0.532803070
0.1,61,0.1,0.1,3,8,0.512226100
0.1,62,0.1,0.1,3,8,0.417472180
0.1,63,0.1,0.1,3,8,0.498767270
0.1,64,0.1,0.1,3,8,0.541254730
0.1,65,0.1,0.1,3,8,0.413515110
0.1,66,0.1,0.1,3,8,0.317025720
0.1,67,0.1,0.1,3,8,0.374055700
0.1,68,0.1,0.1,3,8,0.519659650
0.1,69,0.1,0.1,3,8,0.424875270
0.1,70,0.1,0.1,3,8,0.658948770
0.1,71,0.1,0.1,3,8,0.478070600
0.1,72,0.1,0.1,3,8,0.508434300
0.1,73,0.1,0.1,3,8,0.389327340
0.1,74,0.1,0.1,3,8,0.376710640
0.1,75,0.1,0.1,3,8,0.706846700
0.1,76,0.1,0.1,3,8,0.386015360
0.1,77,0.1,0.1,3,8,0.376482100
0.1,78,0.1,0.1,3,8,0.450980720
0.1,79,0.1,0.1,3,8,0.369264410
0.1,80,0.1,0.1,3,8,0.381190450
0.1,81,0.1,0.1,3,8,0.256231450
0.1,82,0.1,0.1,3,8,0.304787060
0.1,83,0.1,0.1,3,8,0.415943340
0.1,84,0.1,0.1,3,8,0.394611800
0.1,85,0.1,0.1,3,8,0.326296430
0.1,86,0.1,0.1,3,8,0.370579350
0.1,87,0.1,0.1,3,8,0.399765770
0.1,88,0.1,0.1,3,8,0.424445210
0.1,89,0.1,0.1,3,8,0.317458090
0.1,90,0.1,0.1,3,8,0.389528190
0.1,91,0.1,0.1,3,8,0.330826940
0.1,92,0.1,0.1,3,8,0.322034130
0.1,93,0.1,0.1,3,8,0.410222900
0.1,94,0.1,0.1,3,8,0.381071310
0.1,95,0.1,0.1,3,8,0.281873690
0.1,96,0.1,0.1,3,8,0.265378490
0.1,97,0.1,0.1,3,8,0.366434640
0.1,98,0.1,0.1,3,8,0.378843340
0.1,99,0.1,0.1,3,8,0.331006420
0.1,100,0.1,0.1,3,8,0.384889100
