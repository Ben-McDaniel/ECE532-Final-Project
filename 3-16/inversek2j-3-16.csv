Learning Rate, Epochs, Sampling Rate, test data fraction, max layers, max neurons, Error
0.1,1,0.1,0.1,3,16,0.526496220
0.1,2,0.1,0.1,3,16,0.516857960
0.1,3,0.1,0.1,3,16,0.508375390
0.1,4,0.1,0.1,3,16,0.487093200
0.1,5,0.1,0.1,3,16,0.481328100
0.1,6,0.1,0.1,3,16,0.457108650
0.1,7,0.1,0.1,3,16,0.426196330
0.1,8,0.1,0.1,3,16,0.421597970
0.1,9,0.1,0.1,3,16,0.379810100
0.1,10,0.1,0.1,3,16,0.384855610
0.1,11,0.1,0.1,3,16,0.370907130
0.1,12,0.1,0.1,3,16,0.372969880
0.1,13,0.1,0.1,3,16,0.367652230
0.1,14,0.1,0.1,3,16,0.366695470
0.1,15,0.1,0.1,3,16,0.370832710
0.1,16,0.1,0.1,3,16,0.370983170
0.1,17,0.1,0.1,3,16,0.371360830
0.1,18,0.1,0.1,3,16,0.375262350
0.1,19,0.1,0.1,3,16,0.372045510
0.1,20,0.1,0.1,3,16,0.366089480
0.1,21,0.1,0.1,3,16,0.361054870
0.1,22,0.1,0.1,3,16,0.358266640
0.1,23,0.1,0.1,3,16,0.365238220
0.1,24,0.1,0.1,3,16,0.359248510
0.1,25,0.1,0.1,3,16,0.357072090
0.1,26,0.1,0.1,3,16,0.347567390
0.1,27,0.1,0.1,3,16,0.354445940
0.1,28,0.1,0.1,3,16,0.347305040
0.1,29,0.1,0.1,3,16,0.344939400
0.1,30,0.1,0.1,3,16,0.335242680
0.1,31,0.1,0.1,3,16,0.342747540
0.1,32,0.1,0.1,3,16,0.344968040
0.1,33,0.1,0.1,3,16,0.339463330
0.1,34,0.1,0.1,3,16,0.347085560
0.1,35,0.1,0.1,3,16,0.343060110
0.1,36,0.1,0.1,3,16,0.336541220
0.1,37,0.1,0.1,3,16,0.335043720
0.1,38,0.1,0.1,3,16,0.340071760
0.1,39,0.1,0.1,3,16,0.339383950
0.1,40,0.1,0.1,3,16,0.334130260
0.1,41,0.1,0.1,3,16,0.336345840
0.1,42,0.1,0.1,3,16,0.331336100
0.1,43,0.1,0.1,3,16,0.335253620
0.1,44,0.1,0.1,3,16,0.334056950
0.1,45,0.1,0.1,3,16,0.328884400
0.1,46,0.1,0.1,3,16,0.317698210
0.1,47,0.1,0.1,3,16,0.326362940
0.1,48,0.1,0.1,3,16,0.325364940
0.1,49,0.1,0.1,3,16,0.322851640
0.1,50,0.1,0.1,3,16,0.307378630
0.1,51,0.1,0.1,3,16,0.317822830
0.1,52,0.1,0.1,3,16,0.299738600
0.1,53,0.1,0.1,3,16,0.310135340
0.1,54,0.1,0.1,3,16,0.314324550
0.1,55,0.1,0.1,3,16,0.318674140
0.1,56,0.1,0.1,3,16,0.322966580
0.1,57,0.1,0.1,3,16,0.309554550
0.1,58,0.1,0.1,3,16,0.309497380
0.1,59,0.1,0.1,3,16,0.312431260
0.1,60,0.1,0.1,3,16,0.300663780
0.1,61,0.1,0.1,3,16,0.265866310
0.1,62,0.1,0.1,3,16,0.292129710
0.1,63,0.1,0.1,3,16,0.301223010
0.1,64,0.1,0.1,3,16,0.296577820
0.1,65,0.1,0.1,3,16,0.268305870
0.1,66,0.1,0.1,3,16,0.273667170
0.1,67,0.1,0.1,3,16,0.274475350
0.1,68,0.1,0.1,3,16,0.284957140
0.1,69,0.1,0.1,3,16,0.285114850
0.1,70,0.1,0.1,3,16,0.292625140
0.1,71,0.1,0.1,3,16,0.272748140
0.1,72,0.1,0.1,3,16,0.279445680
0.1,73,0.1,0.1,3,16,0.281687960
0.1,74,0.1,0.1,3,16,0.271386780
0.1,75,0.1,0.1,3,16,0.265576850
0.1,76,0.1,0.1,3,16,0.218117530
0.1,77,0.1,0.1,3,16,0.275716370
0.1,78,0.1,0.1,3,16,0.266751960
0.1,79,0.1,0.1,3,16,0.269698640
0.1,80,0.1,0.1,3,16,0.259548740
0.1,81,0.1,0.1,3,16,0.251752260
0.1,82,0.1,0.1,3,16,0.254374490
0.1,83,0.1,0.1,3,16,0.248043040
0.1,84,0.1,0.1,3,16,0.247687300
0.1,85,0.1,0.1,3,16,0.251879530
0.1,86,0.1,0.1,3,16,0.223710360
0.1,87,0.1,0.1,3,16,0.241658390
0.1,88,0.1,0.1,3,16,0.244313090
0.1,89,0.1,0.1,3,16,0.227009530
0.1,90,0.1,0.1,3,16,0.254347700
0.1,91,0.1,0.1,3,16,0.199112490
0.1,92,0.1,0.1,3,16,0.220884930
0.1,93,0.1,0.1,3,16,0.231259130
0.1,94,0.1,0.1,3,16,0.194719290
0.1,95,0.1,0.1,3,16,0.221383140
0.1,96,0.1,0.1,3,16,0.212360490
0.1,97,0.1,0.1,3,16,0.216066210
0.1,98,0.1,0.1,3,16,0.208029480
0.1,99,0.1,0.1,3,16,0.205970680
0.1,100,0.1,0.1,3,16,0.188685760
