Learning Rate, Epochs, Sampling Rate, test data fraction, max layers, max neurons, Error
0.1,1,0.1,0.1,4,16,0.523580630
0.1,2,0.1,0.1,4,16,0.517336360
0.1,3,0.1,0.1,4,16,0.499003530
0.1,4,0.1,0.1,4,16,0.507348000
0.1,5,0.1,0.1,4,16,0.489754210
0.1,6,0.1,0.1,4,16,0.453482890
0.1,7,0.1,0.1,4,16,0.430641350
0.1,8,0.1,0.1,4,16,0.386175790
0.1,9,0.1,0.1,4,16,0.383371470
0.1,10,0.1,0.1,4,16,0.373688530
0.1,11,0.1,0.1,4,16,0.379784070
0.1,12,0.1,0.1,4,16,0.375850100
0.1,13,0.1,0.1,4,16,0.367662800
0.1,14,0.1,0.1,4,16,0.371299400
0.1,15,0.1,0.1,4,16,0.372094930
0.1,16,0.1,0.1,4,16,0.370844540
0.1,17,0.1,0.1,4,16,0.368797170
0.1,18,0.1,0.1,4,16,0.368640300
0.1,19,0.1,0.1,4,16,0.369541180
0.1,20,0.1,0.1,4,16,0.363928370
0.1,21,0.1,0.1,4,16,0.362781150
0.1,22,0.1,0.1,4,16,0.364331570
0.1,23,0.1,0.1,4,16,0.358530070
0.1,24,0.1,0.1,4,16,0.357598290
0.1,25,0.1,0.1,4,16,0.358645380
0.1,26,0.1,0.1,4,16,0.348893100
0.1,27,0.1,0.1,4,16,0.351635080
0.1,28,0.1,0.1,4,16,0.351238060
0.1,29,0.1,0.1,4,16,0.352329890
0.1,30,0.1,0.1,4,16,0.346874300
0.1,31,0.1,0.1,4,16,0.349906810
0.1,32,0.1,0.1,4,16,0.346111630
0.1,33,0.1,0.1,4,16,0.339996470
0.1,34,0.1,0.1,4,16,0.334736810
0.1,35,0.1,0.1,4,16,0.335940040
0.1,36,0.1,0.1,4,16,0.338961280
0.1,37,0.1,0.1,4,16,0.344338960
0.1,38,0.1,0.1,4,16,0.336719200
0.1,39,0.1,0.1,4,16,0.334949730
0.1,40,0.1,0.1,4,16,0.331277540
0.1,41,0.1,0.1,4,16,0.335285530
0.1,42,0.1,0.1,4,16,0.335731990
0.1,43,0.1,0.1,4,16,0.331071360
0.1,44,0.1,0.1,4,16,0.308535190
0.1,45,0.1,0.1,4,16,0.318992290
0.1,46,0.1,0.1,4,16,0.312193380
0.1,47,0.1,0.1,4,16,0.321099620
0.1,48,0.1,0.1,4,16,0.292036800
0.1,49,0.1,0.1,4,16,0.289346890
0.1,50,0.1,0.1,4,16,0.272570810
0.1,51,0.1,0.1,4,16,0.280301770
0.1,52,0.1,0.1,4,16,0.315085070
0.1,53,0.1,0.1,4,16,0.281938570
0.1,54,0.1,0.1,4,16,0.264160860
0.1,55,0.1,0.1,4,16,0.278592050
0.1,56,0.1,0.1,4,16,0.284607000
0.1,57,0.1,0.1,4,16,0.226805780
0.1,58,0.1,0.1,4,16,0.247708920
0.1,59,0.1,0.1,4,16,0.225347010
0.1,60,0.1,0.1,4,16,0.268563500
0.1,61,0.1,0.1,4,16,0.244077320
0.1,62,0.1,0.1,4,16,0.210303450
0.1,63,0.1,0.1,4,16,0.238879740
0.1,64,0.1,0.1,4,16,0.207203580
0.1,65,0.1,0.1,4,16,0.180303600
0.1,66,0.1,0.1,4,16,0.228032610
0.1,67,0.1,0.1,4,16,0.224384660
0.1,68,0.1,0.1,4,16,0.213328830
0.1,69,0.1,0.1,4,16,0.194320910
0.1,70,0.1,0.1,4,16,0.180096570
0.1,71,0.1,0.1,4,16,0.192126570
0.1,72,0.1,0.1,4,16,0.172160480
0.1,73,0.1,0.1,4,16,0.169681280
0.1,74,0.1,0.1,4,16,0.167936660
0.1,75,0.1,0.1,4,16,0.188267200
0.1,76,0.1,0.1,4,16,0.170069740
0.1,77,0.1,0.1,4,16,0.168425310
0.1,78,0.1,0.1,4,16,0.198415350
0.1,79,0.1,0.1,4,16,0.164716740
0.1,80,0.1,0.1,4,16,0.146148860
0.1,81,0.1,0.1,4,16,0.184486220
0.1,82,0.1,0.1,4,16,0.152339240
0.1,83,0.1,0.1,4,16,0.153874760
0.1,84,0.1,0.1,4,16,0.160422360
0.1,85,0.1,0.1,4,16,0.155098500
0.1,86,0.1,0.1,4,16,0.143215020
0.1,87,0.1,0.1,4,16,0.146776490
0.1,88,0.1,0.1,4,16,0.150321170
0.1,89,0.1,0.1,4,16,0.135101900
0.1,90,0.1,0.1,4,16,0.163667170
0.1,91,0.1,0.1,4,16,0.129577660
0.1,92,0.1,0.1,4,16,0.131276300
0.1,93,0.1,0.1,4,16,0.143799720
0.1,94,0.1,0.1,4,16,0.124371140
0.1,95,0.1,0.1,4,16,0.128717540
0.1,96,0.1,0.1,4,16,0.135830480
0.1,97,0.1,0.1,4,16,0.133440450
0.1,98,0.1,0.1,4,16,0.105175950
0.1,99,0.1,0.1,4,16,0.124887050
0.1,100,0.1,0.1,4,16,0.117351040
