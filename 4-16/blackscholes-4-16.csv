Learning Rate, Epochs, Sampling Rate, test data fraction, max layers, max neurons, Error
0.1,1,0.1,0.1,4,16,0.704517900
0.1,2,0.1,0.1,4,16,0.645545190
0.1,3,0.1,0.1,4,16,0.641784880
0.1,4,0.1,0.1,4,16,0.633995690
0.1,5,0.1,0.1,4,16,0.636359730
0.1,6,0.1,0.1,4,16,0.640864990
0.1,7,0.1,0.1,4,16,0.636091140
0.1,8,0.1,0.1,4,16,0.636831170
0.1,9,0.1,0.1,4,16,0.642555580
0.1,10,0.1,0.1,4,16,0.635249520
0.1,11,0.1,0.1,4,16,0.636116800
0.1,12,0.1,0.1,4,16,0.633686700
0.1,13,0.1,0.1,4,16,0.633962670
0.1,14,0.1,0.1,4,16,0.634767780
0.1,15,0.1,0.1,4,16,0.638560120
0.1,16,0.1,0.1,4,16,0.634249710
0.1,17,0.1,0.1,4,16,0.641520100
0.1,18,0.1,0.1,4,16,0.648811500
0.1,19,0.1,0.1,4,16,0.643430110
0.1,20,0.1,0.1,4,16,0.643036270
0.1,21,0.1,0.1,4,16,0.637862580
0.1,22,0.1,0.1,4,16,0.638961560
0.1,23,0.1,0.1,4,16,0.641575500
0.1,24,0.1,0.1,4,16,0.630597190
0.1,25,0.1,0.1,4,16,0.637541310
0.1,26,0.1,0.1,4,16,0.623407200
0.1,27,0.1,0.1,4,16,0.636650030
0.1,28,0.1,0.1,4,16,0.617876270
0.1,29,0.1,0.1,4,16,0.629875360
0.1,30,0.1,0.1,4,16,0.634706610
0.1,31,0.1,0.1,4,16,0.628351900
0.1,32,0.1,0.1,4,16,0.612046760
0.1,33,0.1,0.1,4,16,0.635734880
0.1,34,0.1,0.1,4,16,0.634095250
0.1,35,0.1,0.1,4,16,0.626743300
0.1,36,0.1,0.1,4,16,0.623477460
0.1,37,0.1,0.1,4,16,0.638816010
0.1,38,0.1,0.1,4,16,0.634634430
0.1,39,0.1,0.1,4,16,0.625199260
0.1,40,0.1,0.1,4,16,0.612731610
0.1,41,0.1,0.1,4,16,0.620740250
0.1,42,0.1,0.1,4,16,0.626900560
0.1,43,0.1,0.1,4,16,0.616494190
0.1,44,0.1,0.1,4,16,0.623679230
0.1,45,0.1,0.1,4,16,0.609722870
0.1,46,0.1,0.1,4,16,0.614097400
0.1,47,0.1,0.1,4,16,0.627089080
0.1,48,0.1,0.1,4,16,0.615870570
0.1,49,0.1,0.1,4,16,0.608337980
0.1,50,0.1,0.1,4,16,0.610448790
0.1,51,0.1,0.1,4,16,0.614465320
0.1,52,0.1,0.1,4,16,0.610241450
0.1,53,0.1,0.1,4,16,0.610431400
0.1,54,0.1,0.1,4,16,0.605311240
0.1,55,0.1,0.1,4,16,0.623014580
0.1,56,0.1,0.1,4,16,0.617900170
0.1,57,0.1,0.1,4,16,0.605166570
0.1,58,0.1,0.1,4,16,0.617534470
0.1,59,0.1,0.1,4,16,0.614646470
0.1,60,0.1,0.1,4,16,0.608962310
0.1,61,0.1,0.1,4,16,0.614398920
0.1,62,0.1,0.1,4,16,0.602337740
0.1,63,0.1,0.1,4,16,0.607789820
0.1,64,0.1,0.1,4,16,0.593372290
0.1,65,0.1,0.1,4,16,0.603379240
0.1,66,0.1,0.1,4,16,0.618680820
0.1,67,0.1,0.1,4,16,0.599779670
0.1,68,0.1,0.1,4,16,0.601627220
0.1,69,0.1,0.1,4,16,0.610518790
0.1,70,0.1,0.1,4,16,0.579729530
0.1,71,0.1,0.1,4,16,0.591867030
0.1,72,0.1,0.1,4,16,0.602047580
0.1,73,0.1,0.1,4,16,0.591269370
0.1,74,0.1,0.1,4,16,0.595808130
0.1,75,0.1,0.1,4,16,0.579008820
0.1,76,0.1,0.1,4,16,0.608955040
0.1,77,0.1,0.1,4,16,0.593105070
0.1,78,0.1,0.1,4,16,0.610938180
0.1,79,0.1,0.1,4,16,0.581534000
0.1,80,0.1,0.1,4,16,0.601095020
0.1,81,0.1,0.1,4,16,0.591940210
0.1,82,0.1,0.1,4,16,0.583568580
0.1,83,0.1,0.1,4,16,0.599098350
0.1,84,0.1,0.1,4,16,0.590336890
0.1,85,0.1,0.1,4,16,0.580229050
0.1,86,0.1,0.1,4,16,0.589608190
0.1,87,0.1,0.1,4,16,0.582836830
0.1,88,0.1,0.1,4,16,0.590690520
0.1,89,0.1,0.1,4,16,0.572759070
0.1,90,0.1,0.1,4,16,0.579743890
0.1,91,0.1,0.1,4,16,0.571088020
0.1,92,0.1,0.1,4,16,0.566864160
0.1,93,0.1,0.1,4,16,0.596701240
0.1,94,0.1,0.1,4,16,0.600898870
0.1,95,0.1,0.1,4,16,0.594794300
0.1,96,0.1,0.1,4,16,0.532353830
0.1,97,0.1,0.1,4,16,0.551165330
0.1,98,0.1,0.1,4,16,0.574383040
0.1,99,0.1,0.1,4,16,0.592720280
0.1,100,0.1,0.1,4,16,0.573614380
